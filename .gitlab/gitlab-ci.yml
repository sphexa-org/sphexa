# https://gitlab.com/cscs-ci/ci-testing/webhook-ci/gitlab-runner-docker-jfrog.git
include:
  - remote: 'https://gitlab.com/cscs-ci/recipes/-/raw/master/templates/v2/.cscs.yml'

stages:
  - SPHbase     # no srun (tags=docker_jfrog)
  - SPHbuild    # no srun (tags=docker_jfrog)
  - SPHpull     # on daint
  - SPHtest     # on daint

#{{{ variables
variables:
  docker_jfrog_tag: 'docker_jfrog'
  BASE_CUDA: "${CSCS_REGISTRY_PATH}/sph-exa_base:cuda"
  BASE_HIP: "${CSCS_REGISTRY_PATH}/sph-exa_base:hip"
  BASE_SKIP: "${CSCS_REGISTRY_PATH}/sph-exa_base:skip"
  BUILD_CUDA: "${CSCS_REGISTRY_PATH}/sph-exa_build:cuda"
  # jfrog.svc.cscs.ch/contbuild/testing/anfink/44692846847247 sph-exa_build:cuda
  # jfrog.svc.cscs.ch/contbuild/testing/anfink/4264416954089684 :1.0
  # BUILD_CUDA1: "${CSCS_REGISTRY_PATH}/sph-exa_build:1.0"
  BUILD_HIP: "${CSCS_REGISTRY_PATH}/sph-exa_build:hip"
  DEPS_PATH: 'ftp://ftp.cscs.ch/out/jgp/hpc/containers'
  VERBOSE: 'YES'
  #no: SYSTEM_NAME: '.dom'
  # REBUILD_BASE_IMAGE: 'YES'
  # SARUS_VERBOSE: 'YES'
  # CSCS_REGISTRY_PATH=contbuild/testing/anfink/9590261141699040/
#}}} 

#{{{ sph:base:cuda:
sph:base:cuda:
  tags:
    - ${docker_jfrog_tag}
  stage: SPHbase
  script:
    - echo "DOCKERFILE=$DOCKERFILE"
    - echo "PERSIST_IMAGE_NAME=${PERSIST_IMAGE_NAME}"
  variables:
    # tried with if/else, no success -> must comment/uncomment
    # ---
    # DOCKERFILE: '.gitlab/Dockerfile_1'
    # PERSIST_IMAGE_NAME: "${BASE_CUDA}"
    # ---
    DOCKERFILE: '.gitlab/Dockerfile_skip'
    PERSIST_IMAGE_NAME: "${BASE_SKIP}"
#}}}
#{{{ sph:base:hip:
sph:base:hip:
  tags:
    - ${docker_jfrog_tag}
  stage: SPHbase
  script:
    - echo "DOCKERFILE=$DOCKERFILE"
    - echo "PERSIST_IMAGE_NAME=${PERSIST_IMAGE_NAME}"
  variables:
    # tried with if/else, no success -> must comment/uncomment
    # ---
    # DOCKERFILE: '.gitlab/Dockerfile_hip_1'
    # PERSIST_IMAGE_NAME: "${BASE_HIP}"
    # ---
    DOCKERFILE: '.gitlab/Dockerfile_skip'
    PERSIST_IMAGE_NAME: "${BASE_SKIP}"
#}}}

#{{{ sph:build:cuda:
sph:build:cuda:
  needs: ['sph:base:cuda']
  tags:
    - ${docker_jfrog_tag}
  stage: SPHbuild
  script:
    - echo "PERSIST_IMAGE_NAME=$PERSIST_IMAGE_NAME"
  # image: "${BASE_CUDA}"    
  # /sources
  variables:
    # JG: 'YES'
    DOCKERFILE: '.gitlab/Dockerfile_2'
    PERSIST_IMAGE_NAME: "${BUILD_CUDA}"
    PULL_IMAGE: 'YES'
    CSCS_REGISTRY_LOGIN: 'YES'
#}}}
#{{{ sph:build:hip:
sph:build:hip:
  needs: ['sph:base:hip']
  tags:
    - ${docker_jfrog_tag}
  stage: SPHbuild
  script:
    - echo "PERSIST_IMAGE_NAME=$PERSIST_IMAGE_NAME"
  # image: "${BASE_CUDA}"    
  # /sources
  variables:
    DOCKERFILE: '.gitlab/Dockerfile_hip_2'
    PERSIST_IMAGE_NAME: "${BUILD_HIP}"
    PULL_IMAGE: 'YES'
    CSCS_REGISTRY_LOGIN: 'YES'
#}}}    

#{{{ sph:pull:cuda:
sph:pull:cuda:
  needs: ['sph:build:cuda']
  # tags:
  #   - ${docker_jfrog_tag}
  extends: .daint
  stage: SPHpull
  image: ${BUILD_CUDA}
  script:
    - echo "Pulling image=${BUILD_CUDA}"
    - echo "  CSCS_REGISTRY_PATH=${CSCS_REGISTRY_PATH}"
    - echo "  PERSIST_IMAGE_NAME=${PERSIST_IMAGE_NAME}"
  variables:
    PULL_IMAGE: 'YES'
    CSCS_REGISTRY_LOGIN: 'YES'
    PERSIST_IMAGE_NAME: "${BUILD_CUDA}"
#}}}

#{{{ sph:test:cuda:1:
sph:build:cuda:1:
  needs: ['sph:pull:cuda']
  extends: .daint
  stage: SPHtest
  image: ${BUILD_CUDA}
  script:
    - echo "SLURMD_NODENAME=${SLURMD_NODENAME} SLURM_NODEID=${SLURM_NODEID} SLURM_PROCID=${SLURM_PROCID}"
    - ls -la /usr/local/games/
    - ln -s /usr/local/sbin/hydro/example_data.txt example_data.txt
    - /usr/local/sbin/coord_samples/coordinate_test
    - /usr/local/sbin/performance/scan_perf
    - /usr/local/sbin/coord_samples/coordinate_test
    - /usr/local/sbin/hydro/kernel_tests_ve
    - /usr/local/sbin/hydro/turbulence_tests  
    - /usr/local/sbin/hydro/kernel_tests_std
    - /usr/local/sbin/ryoanji/cpu_unit_tests/ryoanji_cpu_unit_tests
    - /usr/local/sbin/unit/component_units
    - /usr/local/sbin/unit/component_units_omp
    - /usr/local/sbin/performance/peers_perf
  variables:
    USE_MPI: 'NO'
    SLURM_CONSTRAINT: 'gpu'
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 1
    PULL_IMAGE: 'NO'
    # CSCS_REGISTRY_LOGIN: 'YES'
    # SLURM_PARTITION
    # SLURM_TIMELIMIT    
#}}}
#{{{ sph:test:cuda:2:
sph:build:cuda:2:
  needs: ['sph:pull:cuda']
  extends: .daint
  stage: SPHtest
  image: ${BUILD_CUDA}
  script:
    - echo "SLURMD_NODENAME=${SLURMD_NODENAME} SLURM_NODEID=${SLURM_NODEID} SLURM_PROCID=${SLURM_PROCID}"
    - /usr/local/sbin/integration_mpi/domain_2ranks
    - /usr/local/sbin/integration_mpi/exchange_focus
    - /usr/local/sbin/integration_mpi/exchange_halos
    - /usr/local/sbin/integration_mpi/focus_transfer
    - /usr/local/sbin/integration_mpi/globaloctree
    # - /usr/local/sbin/integration_mpi/exchange_halos_gpu # needs 2 gpus
  variables:
    USE_MPI: 'YES'
    SLURM_CONSTRAINT: 'gpu'
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 2
    PULL_IMAGE: 'NO'
    CSCS_REGISTRY_LOGIN: 'YES'
    # SLURM_PARTITION
    # SLURM_TIMELIMIT    
#}}}
#{{{ sph:test:cuda:2cn:
# sph:build:cuda:2cn:
#   needs: ['sph:pull:cuda']
#   extends: .daint
#   stage: SPHtest
#   image: ${BUILD_CUDA}
#   script:
#     - echo "SLURMD_NODENAME=${SLURMD_NODENAME} SLURM_NODEID=${SLURM_NODEID} SLURM_PROCID=${SLURM_PROCID}"
#     - /usr/local/sbin/integration_mpi/assignment_gpu
#     - /usr/local/sbin/integration_mpi/domain_gpu
#     - /usr/local/sbin/integration_mpi/exchange_domain_gpu
#   variables:
#     USE_MPI: 'YES'
#     SLURM_CONSTRAINT: 'gpu'
#     SLURM_JOB_NUM_NODES: 2
#     SLURM_NTASKS: 2
#     PULL_IMAGE: 'NO'
#     CSCS_REGISTRY_LOGIN: 'YES'
#     SLURM_PARTITION: 'debug'
#     # SLURM_TIMELIMIT
#}}}
#{{{ sph:test:cuda:5:
sph:build:cuda:5:
  needs: ['sph:pull:cuda']
  extends: .daint
  stage: SPHtest
  image: ${BUILD_CUDA}
  script:
    - echo "SLURMD_NODENAME=${SLURMD_NODENAME} SLURM_NODEID=${SLURM_NODEID} SLURM_PROCID=${SLURM_PROCID}"
    - /usr/local/sbin/integration_mpi/exchange_domain
    - /usr/local/sbin/integration_mpi/box_mpi
    - /usr/local/sbin/integration_mpi/focus_tree
    - /usr/local/sbin/integration_mpi/treedomain
    - /usr/local/sbin/integration_mpi/domain_nranks
    - /usr/local/sbin/integration_mpi/exchange_general
    - /usr/local/sbin/integration_mpi/exchange_keys
    - /usr/local/sbin/ryoanji/global_upsweep_cpu
  variables:
    USE_MPI: 'YES'
    SLURM_CONSTRAINT: 'gpu'
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 5
    PULL_IMAGE: 'NO'
    CSCS_REGISTRY_LOGIN: 'YES'
    # SLURM_PARTITION
    # SLURM_TIMELIMIT    
#}}}
#{{{ sph:test:cuda:p100:
sph:build:cuda:p100:
  needs: ['sph:pull:cuda']
  extends: .daint
  stage: SPHtest
  image: ${BUILD_CUDA}
  script:
    - echo "SLURMD_NODENAME=${SLURMD_NODENAME} SLURM_NODEID=${SLURM_NODEID} SLURM_PROCID=${SLURM_PROCID}"
    - /usr/local/sbin/performance/hilbert_perf_gpu
    # - /usr/local/sbin/performance/cudaNeighborsTest
    # - /usr/local/sbin/performance/neighbors_test_gpu
    # - /usr/local/sbin/performance/exchange_halos_gpu # moved to integration_mpi
    - /usr/local/sbin/performance/octree_perf_gpu
    - /usr/local/sbin/unit_cuda/component_units_cuda
    - /usr/local/sbin/ryoanji/unit_tests/ryoanji_unit_tests
    - /usr/local/sbin/ryoanji/global_upsweep_gpu
    - /usr/local/sbin/ryoanji/ryoanji_demo/ryoanji_demo 
  variables:
    USE_MPI: 'NO'
    SLURM_CONSTRAINT: 'gpu'
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 1
    PULL_IMAGE: 'NO'
    CSCS_REGISTRY_LOGIN: 'YES'
    # SLURM_PARTITION
    # SLURM_TIMELIMIT    
#}}}
#{{{ sph:test:cuda:sphexa:cpu:
sph:build:cuda:sphexa:cpu:
  needs: ['sph:pull:cuda']
  extends: .daint
  stage: SPHtest
  image: ${BUILD_CUDA}
  script:
    - echo "SLURMD_NODENAME=${SLURMD_NODENAME} SLURM_NODEID=${SLURM_NODEID} SLURM_PROCID=${SLURM_PROCID}"
    - export LD_LIBRARY_PATH=/usr/local/HDF_Group/HDF5/1.13.2/lib:$LD_LIBRARY_PATH
    - echo "sedov:cpu"
    - /usr/local/bin/sphexa --init sedov -s 1 -n 50
    - echo "sedov+ve:cpu"
    - /usr/local/bin/sphexa --init sedov --ve -s 1 -n 50
    - echo "noh:cpu"
    - /usr/local/bin/sphexa --init noh -s 1 -n 50 
  variables:
    USE_MPI: 'YES'
    SLURM_CONSTRAINT: 'gpu'
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 1
    PULL_IMAGE: 'NO'
    CSCS_REGISTRY_LOGIN: 'YES'
    # SLURM_PARTITION
    # SLURM_TIMELIMIT    
#}}}
#{{{ sph:test:cuda:sphexa:gpu:
# TODO: MPICH_RDMA_ENABLED_CUDA=1 LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libcuda.so
sph:build:cuda:sphexa:gpu:
  needs: ['sph:pull:cuda']
  extends: .daint
  stage: SPHtest
  image: ${BUILD_CUDA}
  script:
    - echo "SLURMD_NODENAME=${SLURMD_NODENAME} SLURM_NODEID=${SLURM_NODEID} SLURM_PROCID=${SLURM_PROCID}"
    - export LD_LIBRARY_PATH=/usr/local/HDF_Group/HDF5/1.13.2/lib:$LD_LIBRARY_PATH
    - ln -fs /usr/local/bin/sedov_solution .
    - echo "# --- sedov:gpu"
    - /usr/local/bin/sphexa-cuda --init sedov -s 200 -n 50 -w 200 --outDir /scratch/ --quiet |grep -v "Focus Tree Nodes:"
    - python3 /usr/local/bin/compare_solutions.py -s 200 /scratch/dump_sedov.h5 > /scratch/sedov.rpt
    #
    - echo "# --- noh:gpu"
    - /usr/local/bin/sphexa-cuda --init noh -s 200 -n 50 -w 200 --outDir /scratch/ --quiet |grep -v "Focus Tree Nodes:"
    - python3 /usr/local/bin/compare_noh.py -s 200 /scratch/dump_noh.h5 > /scratch/noh.rpt
    - echo "# --- evrard:gpu"
    - wget --quiet ${DEPS_PATH}/in/glass.h5
    - /usr/local/bin/sphexa-cuda --init evrard --glass ./glass.h5 -s 10 -n 50 -w 10 --outDir /scratch/ --quiet |grep -v "Focus Tree Nodes:"
    - echo "# --- rpt:"
    - cat /scratch/sedov.rpt
    - cat /scratch/noh.rpt
    - pwd
    - ls -la
    - reframe -c /usr/local/games/rfm.py -r -S rpt_path=/scratch
  variables:
    USE_MPI: 'YES'
    SLURM_CONSTRAINT: 'gpu'
    SLURM_JOB_NUM_NODES: 1
    SLURM_NTASKS: 1
    PULL_IMAGE: 'NO'
    CSCS_REGISTRY_LOGIN: 'YES'
    # SLURM_PARTITION
    # SLURM_TIMELIMIT    
#}}}
